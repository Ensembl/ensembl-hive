## Configuration file for the long multiplication pipeline example
#
## Run it like this:
#
# init_pipeline.pl -conf long_mult_pipeline.conf
# 

    # code directories:
my $cvs_root_dir      = $ENV{'HOME'}.'/work';
#my $cvs_root_dir      = $ENV{'HOME'}.'/ensembl_main'; ## for some Compara developers

    # long multiplication pipeline database connection parameters:
my $pipeline_db = {
    -host   => 'compara2',
    -port   => 3306,
    -user   => 'ensadmin',
    -pass   => '#######',
    -dbname => 'long_mult_pipeline',
};

sub dbconn_2_mysql {
    my ($db_conn, $with_db) = @_;

    return "--host=$db_conn->{-host} --port=$db_conn->{-port} "
          ."--user=$db_conn->{-user} --pass=$db_conn->{-pass} "
          .($with_db ? $db_conn->{-dbname} : '');
} 

sub dbconn_2_url {
    my $db_conn = shift @_;

    return "mysql://$db_conn->{-user}:$db_conn->{-pass}\@$db_conn->{-host}:$db_conn->{-port}/$db_conn->{-dbname}";
}


{
        # pass connection parameters into the pipeline initialization script to create adaptors:
    -pipeline_db => $pipeline_db,

        # shell commands that create and possibly pre-fill the pipeline database:
    -pipeline_create_commands => [
        'mysql '.dbconn_2_mysql($pipeline_db, 0)." -e 'CREATE DATABASE $pipeline_db->{-dbname}'",

            # standard eHive tables and procedures:
        'mysql '.dbconn_2_mysql($pipeline_db, 1)." <$cvs_root_dir/ensembl-hive/sql/tables.sql",
        'mysql '.dbconn_2_mysql($pipeline_db, 1)." <$cvs_root_dir/ensembl-hive/sql/procedures.sql",

            # additional tables needed for long multiplication pipeline's operation:
        'mysql '.dbconn_2_mysql($pipeline_db, 1)." -e 'CREATE TABLE intermediate_result (a_multiplier char(40) NOT NULL, digit tinyint NOT NULL, result char(41) NOT NULL, PRIMARY KEY (a_multiplier, digit))'",
        'mysql '.dbconn_2_mysql($pipeline_db, 1)." -e 'CREATE TABLE final_result (a_multiplier char(40) NOT NULL, b_multiplier char(40) NOT NULL, result char(80) NOT NULL, PRIMARY KEY (a_multiplier, b_multiplier))'",

            # name the pipeline to differentiate the submitted processes:
        'mysql '.dbconn_2_mysql($pipeline_db, 1)." -e 'INSERT INTO meta (meta_key, meta_value) VALUES (\"name\", \"lmult\")'",
    ],

    -pipeline_analyses => [
        {   -logic_name => 'start',
            -module     => 'Bio::EnsEMBL::Hive::RunnableDB::LongMult::Start',
            -parameters => {},
            -input_ids => [
                { 'a_multiplier' => '9650516169', 'b_multiplier' => '327358788' },
                { 'a_multiplier' => '327358788', 'b_multiplier' => '9650516169' },
            ],
            -flow_into => [ 'add_together' ],
        },

        {   -logic_name    => 'part_multiply',
            -module        => 'Bio::EnsEMBL::Hive::RunnableDB::LongMult::PartMultiply',
            -parameters    => {},
            -input_ids     => [
                # (jobs for this analysis will be created by the 'start' jobs above)
            ],
        },
        
        {   -logic_name => 'add_together',
            -module     => 'Bio::EnsEMBL::Hive::RunnableDB::LongMult::AddTogether',
            -parameters => {},
            -input_ids => [
                # (jobs for this analysis will be "flown into" from the 'start' jobs above)
            ],
            -wait_for  => [ 'start', 'part_multiply' ], # but we have to wait for both to complete
        },
    ],
};

