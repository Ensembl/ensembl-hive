.. eHive guide to creating pipelines: pipeline configuration file

================
PipeConfig Files
================

PipeConfig basics
=================

A PipeConfig file contains the structure of an eHive workflow:

   - Definitions of analyses and the relationships between them

   - Parameters required by the workflow, optionally with default values

   - Certain eHive configuration options (meta-parameters)

The file itself is a Perl module implementing
``Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf``. This class
defines six interface methods, some or all of which can be overridden
to define a particular workflow:

   - pipeline_analyses() returns a list of hash structures that define analyses and the relationships between them.

   - default_options() returns a hash of defaults for options that the rest of the configuration depends on.

   - pipeline_create_commands() returns a list of strings that will be executed as system commands to set up pipeline dependencies.

   - pipeline_wide_parameters() returns a hash of pipeline-wide parameters, names and values.

   - resource_classes() returns a hash of resource-class definitions.

By convention, PipeConfig files are given names ending in '_conf'.

.. _pipeline-analyses-section:

pipeline_analyses()
===================

.. index:: pipeline_analyses

Every useful PipeConfig will have the pipeline_analyses method, as
this is where the workflow analyses are defined and linked
together. This method returns a list of hashes structures - each hash
defines one analysis. For example:

.. code-block:: perl

  sub pipeline_analyses {
      my ($self) = @_;
      return [
          {   -logic_name => 'first_analysis',
              -comment    => 'this is the first analysis in this workflow',
              -module     => 'Bio::EnsEMBL::Hive::RunnableDB::Dummy',
              -flow_into  => {
                 '1' => 'second_analysis',
              },
          },
          {   -logic_name => 'second_analysis',
              -module     => 'MyCodeBase::RunnableDB::DoSomeWork',
          },
      ];
  }

The code above creates a simple pipeline with two analyses:
'first_analysis' and 'second_analysis'. When a 'first_analysis' job
runs, it will create a dataflow event on branch #1, which will seed a
job of 'second_analysis'. Note that this relationship between
'first_analysis' and 'second_analysis' -- where 'second_analysis' jobs
are seeded by 'first_analysis' jobs -- is entirely created through the
-flow_into block in the 'first_analysis' definition. The order in
which the analysis definitions appear in the list has no effect on
analysis relationships, and is completely arbitrary. That said, it's
generally a good idea to list analysis definitions in a roughly
sequential order to help make the code understandable.

The following directives are available for use in an analysis definition:

.. csv-table:: analysis definition directives
   :header: "Directive", "Required?", "Type", "Description"

   "logic_name", "required", "string", "A name to identify this analysis. Must be unique within the workflow, but is otherwise arbitrary"
   "module", "required", "string", "The classname of the Runnable for this analysis"
   "analysis_capacity", "optional", "integer", "Sets the analysis capacity. Default is unlimited"
   "batch_size", "optional", "integer", "Sets the batch size. Default 1"
   "blocked", "optional", "boolean (0 or 1)", "Seeded jobs of this analysis will start out :hivestatus:`<BLOCKED>[BLOCKED]`."
   can_be_empty, "optional", "boolean (0 or 1)", "If set, then if this analysis has no jobs, it will still block other analyses that ``wait_for`` this analysis."
   "comment", "optional", "string", "A place for documentation. Please be kind to others who will use this pipeline."
   "failed_job_tolerance", "optional", "integer", "Percentage of jobs allowed to fail before the analysis is considered to have failed. Example: ``-failed_job_tolerance => 25`` means that up to 25% of jobs can be :hivestatus:`<FAILED>[FAILED]` before the analysis is considered to be failed. Default 0."
   "flow_into", "optional", "string or arrayref or hashref (see below)", "Directs dataflow events generated by jobs of this analysis."
   "hive_capacity", "optional", "integer", "Sets the reciprocal relative load of this analysis in proportion to the overall hive_capacity. Please see the section covering hive capacity for details."
   "input_ids", "optional", "arrayref", "Sets an input_id hash, or a list of input_id hashes, to seed jobs for this analysis at compile time. See :ref:`the section on seeding jobs <seeding-jobs-into-the-pipeline-database>` for details."
   "language", "optional", "string", "Language of the Runnable: Java, Perl, or Python."
   "max_retry_count", "optional", "integer", "Maximum number of times jobs of this analysis can be retried before they are considered :hivestatus:`<FAILED>[FAILED]`."
   "meadow_type", "optional", "string", "Restricts jobs of this analysis to a particular meadow type. Most commonly used to restrict analyses to run jobs in the LOCAL meadow, but any valid meadow can be given. Note that if a non-local meadow is specified, this will stop automatic failover to LOCAL if LOCAL is the only meadow available."
   "parameters", "optional", "hashref", "Sets analysis-wide parameters and values."
   "priority", "optional", "integer", "Sets relative priority for jobs of this analysis. Workers will claim available jobs from higher priority analyses before claiming jobs of lower priority analyses."
   "rc_name", "optional", "string", "Name of the :ref:`resource_class <resource-classes-overview>` for this analysis."
   "tags", "optional", "arrayref or comma-delimited string", "A tag or set of tags for this analysis."
   "wait_for", "optional", "arrayref or string", "Logic_name, or list of logic_names, of analyses that jobs of this analysis will :ref:`wait for <wait-for-detail>`."

default_options()
=================

A PipeConfig can be created with a set of overridable default options
using the default_options method. This method should return a hashref,
where the keys are option names and the values are option values:

.. code-block:: perl

   sub default_options {
       my ($self) = @_;

       return {
               #First, inherit from the base class. Doing this first
               #allows any defined options to be overridden
               %{ $self->SUPER::default_options() },

               #An example of overriding 'hive_use_param_stack' which is defined
               #in Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf
               'hive_use_param_stack' => 1,

               #An example of setting a new, multilevel default option
               'input_file' => {
                   -file_format   => 'FASTA',
                   -file_contents => 'Nucleotide',
               },
       };
   }

Note that a number of options are set in the base class
``Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf`` -- these may be
overridden by providing a new key value pair in the returned
hashref. Also note that the value for a default option can be another
hashref, creating nested options.

Options set in default_options are available elsewhere in the
PipeConfig via eHive's ``$self->o`` mechanism. For example, to take
the hashref defined as the 'input_file' option above and make it
available to the Runnable ``Some::Runnable`` as a parameter named
'input':

.. code-block:: perl

   sub pipeline_analyses {
       my ($self) = @_;

       return [
           {   -logic_name => 'an_analysis',
               -module     => 'Some::Runnable',
               -parameters => {
                   'input' => $self->o('input_file')
               },
           },
       ];
   }


pipeline_create_commands()
==========================

For some workflows, it may be desirable to perform extra operations at
pipeline creation time. A common example would be adding extra tables
to the hive database. The pipeline_create_commands method is provided
as a place to add these operations that don't fit into the other
methods provided in the PipeConfig interface.

This method should return an arrayref containing ``system``-executable
statements.

For example, the following code adds a 'final_result' table to this
workflow's hive database:

.. code-block:: perl

   sub pipeline_create_commands {
       my ($self) = @_;

       return [
           @{$self->SUPER::pipeline_create_commands},

           $self->db_cmd('CREATE TABLE final_result (inputfile VARCHAR(255) NOT NULL, result DOUBLE PRECISION NOT NULL, PRIMARY KEY (inputfile))'),
       ];
   }


pipeline_wide_parameters()
==========================

The pipeline_wide_parameters method should return a hashref containing
:ref:`parameters <parameters-overview>` available to every analysis in the pipeline. In the
hashref, the hash keys are parameter names, and the hash values are
the parameter values.

.. code-block:: perl

   sub pipeline_wide_parameters {
       my ($self) = @_;

       return {
           # Although Bio::EnsEMBL::Hive::PipeConfig::HiveGeneric_conf
           # does not set any pipeline-wide parameters, a PipeConfig
           # may inherit from a subclass of HiveGeneric_conf that does.
           %{$self->SUPER::pipeline_wide_parameters},

           'my_parameter' => 1,
       };
   }

.. _resource-classes-method:

resource_classes()
==================

Resource classes for a workflow are defined in a PipeConfig's resource_classes method. This method should return a hashref of :ref:`resource class definitions <resource-classes-overview>`.

.. code-block:: perl

   sub resource_classes {
       my ($self) = @_;

       return {
           %{$self->SUPER::resource_classes},
           'high_memory' => { 'LSF' => '-C0 -M16000 -R"rusage[mem=16000]"' },
       };
   }

===============
Dataflow syntax
===============

* At the highest level, the ``-flow_into`` is either a hash
  associating branch tags to targets, or a target directly, in
  which case the branch tag is assumed to be ``1``.
* Branch tags are branch numbers (integers, the same as you would use in
  a Runnable when calling ``dataflow_output_id``) that may be grouped into
  semaphores by adding an arrow and a letter code that identifies the group.
* Essentially, targets are most of the time (local) analysis names, but can
  also be remote analysis names, or accumulator URLs (local or remote).
* Dataflows to these targets can be further controlled in two manners:

  * They can be made conditional using a ``WHEN`` group and a condition. A
    ``WHEN`` group can have as many conditions as you wish, which can
    overlap, and an optional ``ELSE`` clause that acts as a *catch-all*
    (i.e. is activated when no conditions are met).
  * The hash of parameters passed to ``dataflow_output_id`` can be
    transformed before reaching the target with a *template*, which defines
    a new hash of parameters that will be evaluated using eHive's parameter
    substitution mechanism.

Here is a pseudo-BNF definition of the syntax used to model dataflows in
PipeConfig files.

.. code-block:: abnf

  flow-into              = <dataflow-hash> | <target-group>

  dataflow-hash          = "{" <branch-tag> "=>" <target-group> "," * "}"

  branch-tag             = <integer>
                         | <letter> "->" <integer>
                         | <integer> "->" <letter>

  target-group           = <conditional-flow>
                         | <target-names>
                         | <targets-with-template>

  conditional-flow       = "WHEN(" <condition-clause> * <else-clause> ")"

  condition-clause       = <condition> "=>" (<target-names> | <targets-with-template>) ","

  else-clause            = "ELSE" "=>" (<target-names> | <targets-with-template>)

  target-names           = "[" <target-name> * "]"

  targets-with-template  = "{" <target-name> "=>" (<template> | "[" <template> "," * "]" ) "}"

  template               = "undef"
                         | "{" <param-name> "=> "<param-value> "," * "}"

  target-name            = <analysis-name>
                         | <accumulator-url>
                         | <remote-analysis-url>

