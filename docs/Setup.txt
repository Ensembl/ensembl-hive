1- code API needed and executable
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  perl DBI
  Data::UUID (from CPAN.org)
  ensembl
  ensembl-pipeline

1.2 Code checkout

    bioperl code

      cvs -d :ext:bio.perl.org:/home/repository/bioperl co -r branch-07  bioperl-live

    core ensembl code

      cvs -d :ext:cvs.sanger.ac.uk:/nfs/ensembl/cvsroot co ensembl

    ensembl-pipeline code (for Runnables)

      cvs -d :ext:cvs.sanger.ac.uk:/nfs/ensembl/cvsroot co  ensembl-pipeline

    ensembl-hive code

      cvs -d :ext:cvs.sanger.ac.uk:/nfs/ensembl/cvsroot co  ensembl-hive

in tcsh
    setenv BASEDIR   /some/path/to/modules
    setenv PERL5LIB  ${BASEDIR}/ensembl/modules:${BASEDIR}/ensembl-pipeline/modules:${BASEDIR}/ensembl-genepair/modules:${BASEDIR}/bioperl-live

in bash
    BASEDIR=/some/path/to/modules
    PERL5LIB=${BASEDIR}/ensembl/modules:${BASEDIR}/ensembl-pipeline/modules:${BASEDIR}/ensembl-genepair/modules:${BASEDIR}/bioperl-live


2- Configure database
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Pick a mysql instance and create a database

mysqladmin -h ecs2 -P3361 -uensadmin -pxxxx -e "create database hive-test1"

cd ~/src/ensembl_main/ensembl-hive/sql
mysql -h ecs2 -P3361 -uensadmin -pxxxx jessica_hive_test1 < tables.sql

3- Create location where worker and job STDOUT/STDERR is redirected to
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   a) create a working directory with enough disk space to hold hive worker output
     mkdir /nfs/ecs4/work2/ensembl/jessica/data/hive_output/jessica_hive_test1/
   b) insert into meta table
	$outdir = '/nfs/ecs4/work2/ensembl/jessica/data/hive_output/jessica_hive_test1/'
        $dba->get_MetaContainer->store_key_value('hive_output_dir', $outdir);
   
4- Create pipeline graph
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   a) write RunnableDB modules to process data
   b) configure instances of these modules by inserting rows into the analysis table
   c) link dataflow graph of analyses (module instances) by inserting into dataflow_rule table
   d) insert into analysis_ctrl_rule any blocking rules where 'all' of something needs to be
      done before another part of pipeline needs to 'unblock' 
   e) insert starting job(s) into analysis_job table to kick off pipeline

5) Run hive (queen and workers) through a beekeeper
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   eg: lsf_beekeeper.pl -url mysql://ensadmin:xxxx@ecs2:3361/jessica_hive_test1 -loop

