	eHive installation, setup and usage.

1. Download and install the necessary external software:

1.1. Perl 5.6 or higher, since eHive code is written in Perl

	# see http://www.perl.com/download.csp

1.2. MySQL 5.1 or higher

	eHive keeps its state in a MySQL database, so you will need
	(1) a MySQL server installed on the machine where you want to maintain the state and
	(2) MySQL clients installed on the machines where the jobs are to be executed.
	MySQL version 5.1 or higher is recommended to maintain compatibility with Compara pipelines.

	see http://dev.mysql.com/downloads/

1.3. Perl DBI API
	Perl database interface that includes API to MySQL

	# see http://dbi.perl.org/

1.4. Perl UUID API
	eHive uses Universally Unique Identifiers (UUIDs) to identify workers internally.

	# see http://search.cpan.org/dist/Data-UUID/


2. Download and install essential and optional packages from BioPerl and EnsEMBL CVS

2.1. It is advised to have a dedicated directory where EnsEMBL-related packages will be deployed.
Unlike DBI or UUID modules that can be installed system-wide by the system administrator,
you will benefit from full (read+write) access to the EnsEMBL files/directories,
so it is best to install them under your home directory. For example,
	
	$ mkdir $HOME/ensembl_main

It will be convenient to set a variable pointing at this directory for future use:

# using bash syntax:
	$ export ENS_CODE_ROOT="$HOME/ensembl_main"
		#
		# (for best results, append this line to your ~/.bashrc configuration file)

# using [t]csh syntax:
	$ setenv ENS_CODE_ROOT "$HOME/ensembl_main"
		#
		# (for best results, append this line to your ~/.cshrc or ~/.tcshrc configuration file)

2.2. Change into your ensembl codebase directory:

	$ cd $ENS_CODE_ROOT

2.3. Log into the BioPerl CVS server (using "cvs" for password):

	$ cvs -d :pserver:cvs@code.open-bio.org:/home/repository/bioperl login

	# see http://www.bioperl.org/wiki/Using_CVS

2.4. Export the bioperl-live package:

	$ cvs -d :pserver:cvs@code.open-bio.org:/home/repository/bioperl export bioperl-live

2.5. Log into the EnsEMBL CVS server at Sanger (using "CVSUSER" for password):

	$ cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/ensembl login
	Logging in to :pserver:cvsuser@cvs.sanger.ac.uk:2401/cvsroot/ensembl
	CVS password: CVSUSER

2.6. Export ensembl and ensembl-hive CVS modules:

	$ cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/ensembl export ensembl
	$ cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/ensembl export ensembl-hive

2.7. In the likely case you are going to use eHive in the context of Compara pipelines,
	you will also need to install ensembl-compara:

	$ cvs -d :pserver:cvsuser@cvs.sanger.ac.uk:/cvsroot/ensembl export ensembl-compara

2.8. Add new packages to the PERL5LIB variable:

# using bash syntax:
	$ export PERL5LIB=${PERL5LIB}:${ENS_CODE_ROOT}/bioperl-live
	$ export PERL5LIB=${PERL5LIB}:${ENS_CODE_ROOT}/ensembl/modules
	$ export PERL5LIB=${PERL5LIB}:${ENS_CODE_ROOT}/ensembl-hive/modules
	$ export PERL5LIB=${PERL5LIB}:${ENS_CODE_ROOT}/ensembl-compara/modules # optional but recommended, see 2.7.
		#
		# (for best results, append these lines to your ~/.bashrc configuration file)

# using [t]csh syntax:
	$ setenv PERL5LIB  ${PERL5LIB}:${ENS_CODE_ROOT}/bioperl-live
	$ setenv PERL5LIB  ${PERL5LIB}:${ENS_CODE_ROOT}/ensembl/modules
	$ setenv PERL5LIB  ${PERL5LIB}:${ENS_CODE_ROOT}/ensembl-hive/modules
	$ setenv PERL5LIB  ${PERL5LIB}:${ENS_CODE_ROOT}/ensembl-compara/modules # optional but recommended, see 2.7.
		#
		# (for best results, append these lines to your ~/.cshrc or ~/.tcshrc configuration file)


3. Useful files and directories of the eHive repository.

3.1 In ensembl-hive/scripts we keep perl scripts used for controlling the pipelines.
    Adding this directory to your $PATH may make your life easier.

    * init_pipeline.pl is used to create hive databases, populate hive-specific and pipeline-specific tables and load data

    * beekeeper.pl is used to run the pipeline; send 'Workers' to the 'Meadow' to run the jobs of the pipeline

3.2 In ensembl-hive/modules/Bio/EnsEMBL/Hive/PipeConfig we keep example pipeline configuration modules that can be used by init_pipeline.pl .
    A PipeConfig is a parametric module that defines the structure of the pipeline.
    That is, which analyses with what parameters will have to be run and in which order.
    The code for each analysis is contained in a RunnableDB module.
    For some tasks bespoke RunnableDB have to be written, whereas some other problems can be solved by only using 'universal buliding blocks'.
    A typical pipeline is a mixture of both.

3.3 In ensembl-hive/modules/Bio/EnsEMBL/Hive/RunnableDB we keep 'universal building block' RunnableDBs:

    * SystemCmd.pm  is a parameter substitution wrapper for any command line executed by the current shell

    * SqlCmd.pm     is a parameter substitution wrapper for running any MySQL query or a session of linked queries
                    against a particular database (eHive pipeline database by default, but not necessarily)

    * JobFactory.pm is a universal module for dynamically creating batches of same analysis jobs (with different parameters)
                    to be run within the current pipeline

3.4 In ensembl-hive/modules/Bio/EnsEMBL/Hive/RunnableDB/LongMult we keep bespoke RunnableDBs for long multiplication example pipeline.


4   Long multiplication example pipeline.

    Long multiplication pipeline solves a problem of multiplying two very long integer numbers by pretending the computations have to be done in parallel on the farm.
    While performing the task it uses various features of eHive, so by studying this and other examples you can learn how to put together your own pipeines.

4.1 The pipeline is defined in 4 files:

        * ensembl-hive/modules/Bio/EnsEMBL/Hive/RunnableDB/LongMult/Start.pm            splits a multiplication job into sub-tasks and creates corresponding jobs

        * ensembl-hive/modules/Bio/EnsEMBL/Hive/RunnableDB/LongMult/PartMultiply.pm     performs a partial multiplication and stores the intermediate result in a table

        * ensembl-hive/modules/Bio/EnsEMBL/Hive/RunnableDB/LongMult/AddTogether.pm      waits for partial multiplication results to compute and adds them together into final result

        * ensembl-hive/modules/Bio/EnsEMBL/Hive/PipeConfig/LongMult_conf.pm             the pipeline configuration module that links the previous Runnables into one pipeline

4.2 The main part of any PipeConfig file, pipeline_analyses() method, defines the pipeline graph whose nodes are analyses and whose arcs are control and dataflow rules.
    Each analysis hash must have:
        -logic_name     string name by which this analysis is referred to,
        -module         a name of the Runnable module that contains the code to be run (several analyses can use the same Runnable)
    Optionally, it can also have:
        -input_ids      an array of hashes, each hash defining job-specific parameters (if empty it means jobs are created dynamically using dataflow mechanism)
        -parameters     usually a hash of analysis-wide parameters (each such parameter can be overriden by the same name parameter contained in an input_id hash)
        -wait_for       an array of other analyses, *controlling* this one (jobs of this analysis cannot start before all jobs of controlling analyses have completed)
        -flow_into      usually a hash that defines dataflow rules (rules of dynamic job creation during pipeline execution) from this particular analysis.

    The meaning of these parameters should become clearer after some experimentation with the pipeline.


5   Initialization and running the long multiplication pipeline.

5.1 Before running the pipeline you will have to initialize it using init_pipeline.pl script supplying PipeConfig module and the necessary parameters.
    Have another look at LongMult_conf.pm file. The default_options() method returns a hash that pretty much defines what parameters you can/should supply to init_pipeline.pl .
    You will probably need to specify the following:

        $ init_pipeline.pl Bio::EnsEMBL::Hive::PipeConfig::LongMult_conf \
            -ensembl_cvs_root_dir $ENS_CODE_ROOT \
            -pipeline_db -host=<your_mysql_host> \
            -pipeline_db -user=<your_mysql_username> \
            -pipeline_db -user=<your_mysql_password> \

    This should create a fresh eHive database and initalize it with long multiplication pipeline data (the two numbers to be multiplied are taken from defaults).

    Upon successful completion init_pipeline.pl will print several beekeeper commands and
    a mysql command for connecting to the newly created database.
    Copy and run the mysql command in a separate shell session to follow the progress of the pipeline.

5.2 Run the first beekeeper command that contains '-sync' option. This will initialize database's internal stats and determine which jobs can be run.

5.3 Now you have two options: either to run the beekeeper.pl in automatic mode using '-loop' option and wait until it completes,
    or run it in step-by-step mode, initiating every step by separate executions of 'beekeeper.pl ... -run' command.
    We will use the step-by-step mode in order to see what is going on.

5.4 Go to mysql window and check the contents of analysis_job table:

        MySQL> SELECT * FROM analysis_job;

    It will only contain jobs that set up the multiplication tasks in 'READY' mode - meaning 'ready to be taken by workers and executed'.

    Go to the beekeeper window and run the 'beekeeper.pl ... -run' once.
    It will submit a worker to the farm that will at some point get the 'start' job(s).

5.5 Go to mysql window again and check the contents of analysis_job table. Keep checking as the worker may spend some time in 'pending' state.

    After the first worker is done you will see that 'start' jobs are now done and new 'part_multiply' and 'add_together' jobs have been created.
    Also check the contents of 'intermediate_result' table, it should be empty at that moment:

        MySQL> SELECT * from intermediate_result;

    Go back to the beekeeper window and run the 'beekeeper.pl ... -run' for the second time.
    It will submit another worker to the farm that will at some point get the 'part_multiply' jobs.

5.6 Now check both 'analysis_job' and 'intermediate_result' tables again.
    At some moment 'part_multiply' jobs will have been completed and the results will go into 'intermediate_result' table;
    'add_together' jobs are still to be done.
    
    Check the contents of 'final_result' table (should be empty) and run the third and the last round of 'beekeeper.pl ... -run'

5.7 Eventually you will see that all jobs have completed and the 'final_result' table contains final result(s) of multiplication.

